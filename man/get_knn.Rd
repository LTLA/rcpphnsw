% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hnsw.R
\name{get_knn}
\alias{get_knn}
\title{Find Nearest Neighbors and Euclidean Distances}
\usage{
get_knn(X, k = 10, distance = "euclidean", include_self = TRUE,
  M = 16, ef = 200)
}
\arguments{
\item{X}{a numeric matrix of data to add. Each of the n rows is an item in
the index.}

\item{k}{Number of neighbors to return.}

\item{distance}{Type of distance to calculate. One of:
\itemize{
  \item \code{"l2"} Squared L2, i.e. squared Euclidean.
  \item \code{"euclidean"} Euclidean.
  \item \code{"cosine"} Cosine.
  \item \code{"ip"} Inner product: 1 - sum(ai * bi), i.e. the cosine distance
  where the vectors are not normalized. This can lead to negative distances
  and other non-metric behavior.
}}

\item{include_self}{If \code{TRUE}, return the item itself as one of its
\code{k}-neighbors.}

\item{M}{Controls maximum number of neighbors in the zero and above-zero
layers. Higher values lead to better recall and shorter retrieval times, at
the expense of longer indexing time. Suggested range: 5-100 (default: 16).}

\item{ef}{Controls the quality of the graph. Higher values lead to improved
recall at the expense of longer build time. Suggested range: 100-2000
(default: 200).}
}
\value{
a list containing:
\itemize{
  \item \code{idx} an n by k matrix containing the nearest neighbor indices.
  \item \code{dist} an n by k matrix containing the nearest neighbor
  Euclidean distances.
}
}
\description{
A k-nearest neighbor algorithm using the HNSW library
(\url{https://github.com/nmslib/hnsw}).
}
\details{
This function also demonstrates how to use the minimal interface to HNSW to
do something non-trivial. The class used, "HnswL2", uses the "Squared L2"
distance, which is the square of the Euclidean distance. This function takes
care of the square root for you, so returns actual Euclidean distances.
}
\examples{
get_knn(as.matrix(iris[, -5]), k = 10)
}
